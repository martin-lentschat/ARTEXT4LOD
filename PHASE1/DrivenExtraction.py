# coding: utf-8from __future__ import unicode_literals"""Created on Tue May 9 14:07:38 2017For: UMR TETIS RESEARCH UNITAuthor: Gaurav_Shrivastava"""import jsonimport osimport refrom collections import defaultdictimport nltkfrom nltk.corpus import stopwordsfrom nltk.corpus import wordnet as wn'''for generating the proper tag for lemmatizer'''# test si nomdef is_noun(tag):    return tag in ['NN', 'NNS', 'NNP', 'NNPS']# test si verbedef is_verb(tag):    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']# test si adverbdef is_adverb(tag):    return tag in ['RB', 'RBR', 'RBS']# test si adjectifdef is_adjective(tag):    return tag in ['JJ', 'JJR', 'JJS']def penn_to_wn(tag):    if is_adjective(tag):        return wn.ADJ    elif is_noun(tag):        return wn.NOUN    elif is_adverb(tag):        return wn.ADV    elif is_verb(tag):        return wn.VERB    return None'''converts the text corpora into list of tokens'''def pre_processing(document):    sentences = nltk.sent_tokenize(document)    sentences = [nltk.word_tokenize(sent) for sent in sentences]    # sentences = [nltk.pos_tag(sent) for sent in sentences]    return sentences'''Part of speech tagging for documents to preserve context for lemmatizer'''def pos_tag(document):    sentences = nltk.sent_tokenize(document)    sentences = [nltk.word_tokenize(sent) for sent in sentences]    sentences = [nltk.pos_tag(sent) for sent in sentences]    return sentences'''Utility function to flatten list of lists into single list'''def flatten(lists):    newlist = []    for item in lists:        for index in range(len(item)):            newlist.append(item[index])    return newlist'''reading text corpus'''def reading_valorcarn_corpus(filename):    f = open(filename)    string = f.read()    docs = string.split("##########END##########")    return docs'''reading list of terms'''def read_file(filename):    word = []    if '.txt' in filename:        f = open(filename)        string = f.read()        word = string.split('\n')    elif '.owl' in filename:        f = open(filename)        string = f.read()        labels2 = re.findall('skos:altLabel xml:lang="en".+?</', string, re.DOTALL)        labels1 = re.findall('skos:prefLabel xml:lang="en".+?</', string, re.DOTALL)        for i in [*labels1, *labels2]:            word.append(re.sub('[\n ]+', ' ', i.split('<')[0].split('>')[-1]))    return word"""Normalises words to lowercase, stems and lemmatizes it.(input is a word)"""def normalise(words, tags):    normalised = defaultdict(list)    counter = 0    for i in range(len(words)):        word = words[i].lower()        #        if penn_to_wn(tags[i][1]):            word = lemmatizer.lemmatize(word, pos=penn_to_wn(tags[i][1]))        word = stemmer.stem(word)        normalised[word].append(counter)        counter = counter + 1    return normalised"""Normalises words to lowercase, stems and lemmatizes it.(input is list of words)"""def list_normalize(words):    normalised = []    for i in range(len(words)):        word = words[i].lower()        word = lemmatizer.lemmatize(word)        word = stemmer.stem(word)        normalised.append(word)    return list(set(normalised))def normalize(word):    word = lemmatizer.lemmatize(word)  # ,pos = penn_to_wn(tags[i][1]))    word = stemmer.stem(word)    return worddef extract_singles_variation(words, norm_dict, filtered_words):    singles = defaultdict(list)    for word in words:        if word and ' ' not in word:            temporary_extract = extract_singles(word, norm_dict, filtered_words)            if temporary_extract:                singles[word].append(list(set(temporary_extract)))            # else:            #     singles[word].append("#NONE#")    return singlesdef extract_singles(word, norm_dict, filtered_words):    word = normalize(word)    if word in norm_dict:        temp = norm_dict[word]        word_list = []        for entry in temp:            word_list.append(filtered_words[entry])        return word_list    return Nonedef extract_couples(word1, word2, norm_dict, filtered_words, k, o):  # find root for both words    word1 = normalize(word1)    word2 = normalize(word2)    if word1 in norm_dict:        if word2 in norm_dict:            word_set = set([])  # extract the occurrences of the root word in the corpus            instance1 = norm_dict[word1]            instance2 = norm_dict[word2]  # matching the word with k- words occurring between the root of the two words            extracted = matching(instance1, instance2, k, o)            for extract in extracted:                terms = terms_extract(extract, filtered_words)                word_set.add(terms)            return list(word_set)    return Nonedef terms_extract(extract, filtered_words):    terminology = ''    for entry in range(extract[0], extract[1]):        terminology = terminology + filtered_words[entry] + ' '    terminology = terminology + filtered_words[extract[1]]    return terminologydef matching(array1, array2, k, o):    extracted = []    if o:        for entry in array1:            for i in range(entry, entry + k):                if i in array2:                    mini = min([entry, i])                    maxi = max([entry, i])                    extracted.append([mini, maxi])                    break    else:        for entry in array1:            for i in range(entry - k, entry + k):                if i in array2:                    mini = min([entry, i])                    maxi = max([entry, i])                    extracted.append([mini, maxi])                    break    return extracteddef extract_multi_variation(words, norm_dict, filtered_words, k, o, c):    multi = defaultdict(list)    for word in words:        if word and ' ' in word:            temp = word.split(' ')            if len(temp) > 1:                for i in range(0, len(temp) - 1):                    word1 = temp[i]                    for j in range(1 + i, len(temp)):                        word2 = temp[j]                        temporary_extract = extract_couples(word1, word2, norm_dict, filtered_words, k+j-1, o)                        if temporary_extract:                            if c and valid_compo(temporary_extract, temp):                                multi[word].append([x for x in temporary_extract if not re.findall('[,;!?._(]', x)])    return {k: v for k, v in multi.items() if v}def valid_compo(temporary_extract, temp):    res = []    for t in temporary_extract:        valid = True        for w in temp:            if w not in t:                valid = False        if valid:            res.append(t)    return resdef reading_corpus(path):    string = ''    for f in os.listdir(path):        if '.txt' in f:            file = open(path+'/'+f,  encoding="utf8")            string += file.read()    return stringdef merge_two_dicts(x, y):    z = x.copy()  # start with x's keys and values    z.update(y)  # modifies z with y's keys and values & returns None    return zdef cleanNONE(dico):    res = {}    for i in dico:        if dico[i] != "#NONE#" and dico[i] != i:            res[i] = [x for y in dico[i] for x in y if x != i]    return resdef str2bool(v):    if isinstance(v, bool):        return v    elif isinstance(v, str):        return v.lower() in ('yes', 'true', 't', '1', '42', 'roger')def main2(name, path, lists, k_value, order, partial, lemma):    global stemmer, lemmatizer    stemmer = nltk.stem.porter.PorterStemmer()    lemmatizer = nltk.WordNetLemmatizer()    docs = reading_corpus(path)    # print('k_value : '+str(k_value))    # print('ordered : '+str(order))    # print('partiel : ' + str(partial))    # print('lemmatized : '+str(lemma))    if lemma:        lists = list_normalize(lists)    print('Research of ' + str(len(lists)) + ' terms')    # print('Pre-Processing')    words = pre_processing(docs)    tags = pos_tag(docs)    words = flatten(words)    tags = flatten(tags)    # print('Stemming/Lemmatizing ...')    filtered_words = [word for word in words if word not in stopwords.words('english')]    filtered_tags = [word for word in tags if word[0] not in stopwords.words('english')]    normalised = normalise(filtered_words, filtered_tags)    singles = extract_singles_variation(lists, normalised, filtered_words)    # print('Singles results : '+str(len(singles))+' entries, '+str(sum([len(x) for x in singles.values()]))+' variations')    normalised = normalise(words, tags)    multi = extract_multi_variation(lists, normalised, words, k_value, order, partial)    # print('Multi results : '+str(len(multi))+' entries, '+str(sum([len(x) for x in multi.values()]))+' variations')    open('work_files/'+name+'_driven_extraction.json', 'w').write(json.dumps(cleanNONE(merge_two_dicts(singles, multi)),                                                                             indent=4))    return cleanNONE(merge_two_dicts(singles, multi))def main():    print('DrivenExtraction')if __name__ == '__main__':    main()